{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "huggingfacehub_api_key= os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import deque\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "from langchain import LLMChain, OpenAI, PromptTemplate\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import BaseLLM\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains.base import Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your embedding model\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "# Initialize the vectorstore as empty\n",
    "import faiss\n",
    "\n",
    "embedding_size = 1536\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskCreationChain(LLMChain):\n",
    "    \"\"\"Chain to generates tasks.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n",
    "        \"\"\"Get the response parser.\"\"\"\n",
    "        task_creation_template = (\n",
    "            \"You are an task creation AI that uses the result of an execution agent\"\n",
    "            \" to create new tasks with the following objective: {objective},\"\n",
    "            \" The last completed task has the result: {result}.\"\n",
    "            \" This result was based on this task description: {task_description}.\"\n",
    "            \" These are incomplete tasks: {incomplete_tasks}.\"\n",
    "            \" Based on the result, create new tasks to be completed\"\n",
    "            \" by the AI system that do not overlap with incomplete tasks.\"\n",
    "            \" Return the tasks as an array.\"\n",
    "        )\n",
    "        prompt = PromptTemplate(\n",
    "            template=task_creation_template,\n",
    "            input_variables=[\n",
    "                \"result\",\n",
    "                \"task_description\",\n",
    "                \"incomplete_tasks\",\n",
    "                \"objective\",\n",
    "            ],\n",
    "        )\n",
    "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskPrioritizationChain(LLMChain):\n",
    "    \"\"\"Chain to prioritize tasks.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n",
    "        \"\"\"Get the response parser.\"\"\"\n",
    "        task_prioritization_template = (\n",
    "            \"You are an task prioritization AI tasked with cleaning the formatting of and reprioritizing\"\n",
    "            \" the following tasks: {task_names}.\"\n",
    "            \" Consider the ultimate objective of your team: {objective}.\"\n",
    "            \" Do not remove any tasks. Return the result as a numbered list, like:\"\n",
    "            \" #. First task\"\n",
    "            \" #. Second task\"\n",
    "            \" Start the task list with number {next_task_id}.\"\n",
    "        )\n",
    "        prompt = PromptTemplate(\n",
    "            template=task_prioritization_template,\n",
    "            input_variables=[\"task_names\", \"next_task_id\", \"objective\"],\n",
    "        )\n",
    "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.tools.file_management.write import WriteFileTool\n",
    "from langchain.tools.file_management.read import ReadFileTool\n",
    "from langchain.utilities import PythonREPL\n",
    "from langchain.tools import DuckDuckGoSearchRun \n",
    "from langchain.tools.file_management import (\n",
    "    ReadFileTool,\n",
    "    CopyFileTool,\n",
    "    DeleteFileTool,\n",
    "    MoveFileTool,\n",
    "    WriteFileTool,\n",
    "    ListDirectoryTool,\n",
    ")\n",
    "from langchain.agents.agent_toolkits import FileManagementToolkit\n",
    "from tempfile import TemporaryDirectory\n",
    "from langchain.tools import ShellTool\n",
    "from langchain.agents import load_tools\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make a temporary directory to avoid clutter\n",
    "working_directory = TemporaryDirectory()\n",
    "#Shell Tool\n",
    "shell_tool = ShellTool()\n",
    "requests_tools = load_tools([\"requests_all\"])\n",
    "\n",
    "#DuckDuckGo Class\n",
    "duckduck_search = DuckDuckGoSearchRun()\n",
    "\n",
    "#Python REPL Class\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "shell_tool = Tool(\n",
    "    name=\"shell\",\n",
    "    description=\"This tool gives you access to a system's shell. Can be used to run shell commands\",\n",
    "    func=shell_tool.run\n",
    ")\n",
    "\n",
    "\n",
    "#DuckDuckGo Tool\n",
    "duckduck_tool=Tool(\n",
    "        name = \"DuckDuckGo\",\n",
    "        func=duckduck_search.run,\n",
    "        description=\"useful for when you need to answer questions about current events. You should ask targeted questions\"\n",
    ")\n",
    "\n",
    "#Python REPL Agent Tool Class\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "    func=python_repl.run\n",
    ")\n",
    "toolkit = FileManagementToolkit(root_dir=str(working_directory.name)) # If you don't provide a root_dir, operations will default to the current working directory\n",
    "directory_tools = toolkit.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from pydantic import Field\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain, BaseCombineDocumentsChain\n",
    "\n",
    "def _get_text_splitter():\n",
    "    return RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap  = 20,\n",
    "        length_function = len,\n",
    "    )\n",
    "\n",
    "\n",
    "class WebpageQATool(BaseTool):\n",
    "    name = \"query_webpage\"\n",
    "    description = \"Browse a webpage and retrieve the information relevant to the question.\"\n",
    "    text_splitter: RecursiveCharacterTextSplitter = Field(default_factory=_get_text_splitter)\n",
    "    qa_chain: BaseCombineDocumentsChain\n",
    "    \n",
    "    def _run(self, url: str, question: str) -> str:\n",
    "        \"\"\"Useful for browsing websites and scraping the text information.\"\"\"\n",
    "        result = browse_web_page.run(url)\n",
    "        docs = [Document(page_content=result, metadata={\"source\": url})]\n",
    "        web_docs = self.text_splitter.split_documents(docs)\n",
    "        results = []\n",
    "        # TODO: Handle this with a MapReduceChain\n",
    "        for i in range(0, len(web_docs), 4):\n",
    "            input_docs = web_docs[i:i+4]\n",
    "            window_result = self.qa_chain({\"input_documents\": input_docs, \"question\": question}, return_only_outputs=True)\n",
    "            results.append(f\"Response from window {i} - {window_result}\")\n",
    "        results_docs = [Document(page_content=\"\\n\".join(results), metadata={\"source\": url})]\n",
    "        return self.qa_chain({\"input_documents\": results_docs, \"question\": question}, return_only_outputs=True)\n",
    "    \n",
    "    async def _arun(self, url: str, question: str) -> str:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_website_tool = WebpageQATool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tools = [\n",
    "    WriteFileTool(),\n",
    "    ReadFileTool(),\n",
    "    repl_tool,\n",
    "    duckduck_tool, \n",
    "    #directory_tools, \n",
    "    shell_tool, \n",
    "    #requests_tool,\n",
    "    query_website_tool\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"You are an AI who performs one task based on the following objective: {objective}. Take into account these previously completed tasks: {context}.\"\"\"\n",
    "suffix = \"\"\"Question: {task}\n",
    "{agent_scratchpad}\"\"\"\n",
    "prompt = ZeroShotAgent.create_prompt(\n",
    "    tools,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"objective\", \"task\", \"context\", \"agent_scratchpad\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_task(\n",
    "    task_creation_chain: LLMChain,\n",
    "    result: Dict,\n",
    "    task_description: str,\n",
    "    task_list: List[str],\n",
    "    objective: str,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Get the next task.\"\"\"\n",
    "    incomplete_tasks = \", \".join(task_list)\n",
    "    response = task_creation_chain.run(\n",
    "        result=result,\n",
    "        task_description=task_description,\n",
    "        incomplete_tasks=incomplete_tasks,\n",
    "        objective=objective,\n",
    "    )\n",
    "    new_tasks = response.split(\"\\n\")\n",
    "    return [{\"task_name\": task_name} for task_name in new_tasks if task_name.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritize_tasks(\n",
    "    task_prioritization_chain: LLMChain,\n",
    "    this_task_id: int,\n",
    "    task_list: List[Dict],\n",
    "    objective: str,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Prioritize tasks.\"\"\"\n",
    "    task_names = [t[\"task_name\"] for t in task_list]\n",
    "    next_task_id = int(this_task_id) + 1\n",
    "    response = task_prioritization_chain.run(\n",
    "        task_names=task_names, next_task_id=next_task_id, objective=objective\n",
    "    )\n",
    "    new_tasks = response.split(\"\\n\")\n",
    "    prioritized_task_list = []\n",
    "    for task_string in new_tasks:\n",
    "        if not task_string.strip():\n",
    "            continue\n",
    "        task_parts = task_string.strip().split(\".\", 1)\n",
    "        if len(task_parts) == 2:\n",
    "            task_id = task_parts[0].strip()\n",
    "            task_name = task_parts[1].strip()\n",
    "            prioritized_task_list.append({\"task_id\": task_id, \"task_name\": task_name})\n",
    "    return prioritized_task_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_top_tasks(vectorstore, query: str, k: int) -> List[str]:\n",
    "    \"\"\"Get the top k tasks based on the query.\"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    if not results:\n",
    "        return []\n",
    "    sorted_results, _ = zip(*sorted(results, key=lambda x: x[1], reverse=True))\n",
    "    return [str(item.metadata[\"task\"]) for item in sorted_results]\n",
    "\n",
    "\n",
    "def execute_task(\n",
    "    vectorstore, execution_chain: LLMChain, objective: str, task: str, k: int = 5\n",
    ") -> str:\n",
    "    \"\"\"Execute a task.\"\"\"\n",
    "    context = _get_top_tasks(vectorstore, query=objective, k=k)\n",
    "    return execution_chain.run(objective=objective, context=context, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyAGI(Chain, BaseModel):\n",
    "    \"\"\"Controller model for the BabyAGI agent.\"\"\"\n",
    "\n",
    "    task_list: deque = Field(default_factory=deque)\n",
    "    task_creation_chain: TaskCreationChain = Field(...)\n",
    "    task_prioritization_chain: TaskPrioritizationChain = Field(...)\n",
    "    execution_chain: AgentExecutor = Field(...)\n",
    "    task_id_counter: int = Field(1)\n",
    "    vectorstore: VectorStore = Field(init=False)\n",
    "    max_iterations: Optional[int] = None\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def add_task(self, task: Dict):\n",
    "        self.task_list.append(task)\n",
    "\n",
    "    def print_task_list(self):\n",
    "        print(\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\\033[0m\")\n",
    "        for t in self.task_list:\n",
    "            print(str(t[\"task_id\"]) + \": \" + t[\"task_name\"])\n",
    "\n",
    "    def print_next_task(self, task: Dict):\n",
    "        print(\"\\033[92m\\033[1m\" + \"\\n*****NEXT TASK*****\\n\" + \"\\033[0m\\033[0m\")\n",
    "        print(str(task[\"task_id\"]) + \": \" + task[\"task_name\"])\n",
    "\n",
    "    def print_task_result(self, result: str):\n",
    "        print(\"\\033[93m\\033[1m\" + \"\\n*****TASK RESULT*****\\n\" + \"\\033[0m\\033[0m\")\n",
    "        print(result)\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return [\"objective\"]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return []\n",
    "\n",
    "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Run the agent.\"\"\"\n",
    "        objective = inputs[\"objective\"]\n",
    "        first_task = inputs.get(\"first_task\", \"Make a todo list\")\n",
    "        self.add_task({\"task_id\": 1, \"task_name\": first_task})\n",
    "        num_iters = 0\n",
    "        while True:\n",
    "            if self.task_list:\n",
    "                self.print_task_list()\n",
    "\n",
    "                # Step 1: Pull the first task\n",
    "                task = self.task_list.popleft()\n",
    "                self.print_next_task(task)\n",
    "\n",
    "                # Step 2: Execute the task\n",
    "                result = execute_task(\n",
    "                    self.vectorstore, self.execution_chain, objective, task[\"task_name\"]\n",
    "                )\n",
    "                this_task_id = int(task[\"task_id\"])\n",
    "                self.print_task_result(result)\n",
    "\n",
    "                # Step 3: Store the result in Pinecone\n",
    "                result_id = f\"result_{task['task_id']}\"\n",
    "                self.vectorstore.add_texts(\n",
    "                    texts=[result],\n",
    "                    metadatas=[{\"task\": task[\"task_name\"]}],\n",
    "                    ids=[result_id],\n",
    "                )\n",
    "\n",
    "                # Step 4: Create new tasks and reprioritize task list\n",
    "                new_tasks = get_next_task(\n",
    "                    self.task_creation_chain,\n",
    "                    result,\n",
    "                    task[\"task_name\"],\n",
    "                    [t[\"task_name\"] for t in self.task_list],\n",
    "                    objective,\n",
    "                )\n",
    "                for new_task in new_tasks:\n",
    "                    self.task_id_counter += 1\n",
    "                    new_task.update({\"task_id\": self.task_id_counter})\n",
    "                    self.add_task(new_task)\n",
    "                self.task_list = deque(\n",
    "                    prioritize_tasks(\n",
    "                        self.task_prioritization_chain,\n",
    "                        this_task_id,\n",
    "                        list(self.task_list),\n",
    "                        objective,\n",
    "                    )\n",
    "                )\n",
    "            num_iters += 1\n",
    "            if self.max_iterations is not None and num_iters == self.max_iterations:\n",
    "                print(\n",
    "                    \"\\033[91m\\033[1m\" + \"\\n*****TASK ENDING*****\\n\" + \"\\033[0m\\033[0m\"\n",
    "                )\n",
    "                break\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls, llm: BaseLLM, vectorstore: VectorStore, verbose: bool = False, **kwargs\n",
    "    ) -> \"BabyAGI\":\n",
    "        \"\"\"Initialize the BabyAGI Controller.\"\"\"\n",
    "        task_creation_chain = TaskCreationChain.from_llm(llm, verbose=verbose)\n",
    "        task_prioritization_chain = TaskPrioritizationChain.from_llm(\n",
    "            llm, verbose=verbose\n",
    "        )\n",
    "        llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        tool_names = [tool.name for tool in tools]\n",
    "        agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\n",
    "        agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "            agent=agent, tools=tools, verbose=True\n",
    "        )\n",
    "        return cls(\n",
    "            task_creation_chain=task_creation_chain,\n",
    "            task_prioritization_chain=task_prioritization_chain,\n",
    "            execution_chain=agent_executor,\n",
    "            vectorstore=vectorstore,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECTIVE = \"Write a report on ecommerce trends in Africa in 2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ./ggml-gpt4all-j-v1.3-groovy.bin\n",
      "gptj_model_load: loading model from './ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
      "gptj_model_load: n_vocab = 50400\n",
      "gptj_model_load: n_ctx   = 2048\n",
      "gptj_model_load: n_embd  = 4096\n",
      "gptj_model_load: n_head  = 16\n",
      "gptj_model_load: n_layer = 28\n",
      "gptj_model_load: n_rot   = 64\n",
      "gptj_model_load: f16     = 2\n",
      "gptj_model_load: ggml ctx size = 5401.45 MB\n",
      "gptj_model_load: kv self size  =  896.00 MB\n",
      "gptj_model_load: ................................... done\n",
      "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n"
     ]
    }
   ],
   "source": [
    "#llm = OpenAI(temperature=0)\n",
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "#Gpt4all Model\n",
    "model_path = \"./ggml-gpt4all-j-v1.3-groovy.bin\"\n",
    "llm = GPT4All(model=model_path, n_ctx=2048, callbacks=callbacks, temp=0.2, backend=\"gptj\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging of LLMChains\n",
    "verbose = False\n",
    "# If None, will keep on going forever\n",
    "max_iterations: Optional[int] = 3\n",
    "baby_agi = BabyAGI.from_llm(\n",
    "    llm=llm, vectorstore=vectorstore, verbose=verbose, max_iterations=max_iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[1m\n",
      "*****TASK LIST*****\n",
      "\u001b[0m\u001b[0m\n",
      "1: Make a todo list\n",
      "\u001b[92m\u001b[1m\n",
      "*****NEXT TASK*****\n",
      "\u001b[0m\u001b[0m\n",
      "1: Make a todo list\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function LLModel._prompt_callback at 0x7fd8eb2119e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hmazomba/miniconda3/envs/cedric/lib/python3.11/site-packages/gpt4all/pyllmodel.py\", line 323, in _prompt_callback\n",
      "    @staticmethod\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "baby_agi({\"objective\": OBJECTIVE})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
