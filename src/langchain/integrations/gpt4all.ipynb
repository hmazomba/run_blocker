{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = (\n",
    "    \"../../../../../Models/GPT4ALL/llama-2-7b-chat.ggmlv3.q4_0.bin\"  # replace with your desired local file path\n",
    ")\n",
    "\n",
    "# import requests\n",
    "\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Example model. Check https://github.com/nomic-ai/gpt4all for the latest models.\n",
    "# url = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n",
    "\n",
    "# # send a GET request to the URL to download the file. Stream since it's large\n",
    "# response = requests.get(url, stream=True)\n",
    "\n",
    "# # open the file in binary mode and write the contents of the response to it in chunks\n",
    "# # This is a large file, so be prepared to wait.\n",
    "# with open(local_path, 'wb') as f:\n",
    "#     for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
    "#         if chunk:\n",
    "#             f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ../../../../../Models/GPT4ALL/llama-2-7b-chat.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../../../../../Models/GPT4ALL/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
      "llama_model_load_internal: mem required  = 5407.71 MB (+ 1026.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)\n",
    "\n",
    "# If you want to use a custom model add the backend parameter\n",
    "# Check https://docs.gpt4all.io/gpt4all_python.html for supported backends\n",
    "#llm = GPT4All(model=local_path, backend=\"gptj\", callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The term \"virtual\" can have different meanings depending on the context, but in computing it usually refers to something that exists or operates remotely through technology rather than physically present at a particular location. Here are some examples of how \"virtual\" is used in computing:\n",
      "1. Virtual machine (VM): A virtual machine is an emulation of a physical computer within another computer, allowing multiple operating systems and applications to run on a single physical host without the need for separate hardware for each environment. For example, you can install Windows 10 as a VM inside macOS or Linux using software like VirtualBox or VMware Workstation.\n",
      "2. Virtualization: Virtualization is the technology that allows multiple virtual machines (VMs) to run on top of a single host operating system without conflict between them. This enables organizations to consolidate their computing resources and reduce hardware costs by running more applications on fewer physical servers. Examples include VMware, Hyper-V, or KVM.\n",
      "3. Virtualization software: As mentioned earlier, virtualization software creates an emulation of a computer within another computer. Popular examples include Oracle's VirtualBox, VMware Workstation, and Microsoft's Hyper-V. These tools allow you to"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The term \"virtual\" can have different meanings depending on the context, but in computing it usually refers to something that exists or operates remotely through technology rather than physically present at a particular location. Here are some examples of how \"virtual\" is used in computing:\\n1. Virtual machine (VM): A virtual machine is an emulation of a physical computer within another computer, allowing multiple operating systems and applications to run on a single physical host without the need for separate hardware for each environment. For example, you can install Windows 10 as a VM inside macOS or Linux using software like VirtualBox or VMware Workstation.\\n2. Virtualization: Virtualization is the technology that allows multiple virtual machines (VMs) to run on top of a single host operating system without conflict between them. This enables organizations to consolidate their computing resources and reduce hardware costs by running more applications on fewer physical servers. Examples include VMware, Hyper-V, or KVM.\\n3. Virtualization software: As mentioned earlier, virtualization software creates an emulation of a computer within another computer. Popular examples include Oracle\\'s VirtualBox, VMware Workstation, and Microsoft\\'s Hyper-V. These tools allow you to'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is a virtual server in computing?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, we need to install the required libraries:\n",
      "```\n",
      "pip install pyttsx3\n",
      "pip install speechrecognition\n",
      "```\n",
      "Now, let's create a simple recognizer in Python using `pyttsx3` and `SpeechRecognition`:\n",
      "```python\n",
      "import pyttsx3\n",
      "from speech_recognition import SpeechRecognizer\n",
      "\n",
      "# Set up the voice\n",
      "engine = pyttsx3.init()\n",
      "voice='slt_kevin' # or any other voice you prefer\n",
      "engine.setProperty('voice', voice)\n",
      "\n",
      "# Create a recognizer instance\n",
      "recognizer = SpeechRecognizer(engine=engine, vocabulary='english')\n",
      "\n",
      "# Start the recognition process\n",
      "while True:\n",
      "    try:\n",
      "        # Get audio input from microphone or file\n",
      "        audio, _ = recognizer.record()\n",
      "        \n",
      "        # Recognize speech using Google Cloud Speech-to-Text API\n",
      "        transcript = recognizer.recognize_speech(audio)\n",
      "        \n",
      "        # Print the recognized text\n",
      "        print(transcript)\n",
      "    except Exception as e:\n",
      "        print('Error:', e)\n",
      "```\n",
      "In"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" First, we need to install the required libraries:\\n```\\npip install pyttsx3\\npip install speechrecognition\\n```\\nNow, let's create a simple recognizer in Python using `pyttsx3` and `SpeechRecognition`:\\n```python\\nimport pyttsx3\\nfrom speech_recognition import SpeechRecognizer\\n\\n# Set up the voice\\nengine = pyttsx3.init()\\nvoice='slt_kevin' # or any other voice you prefer\\nengine.setProperty('voice', voice)\\n\\n# Create a recognizer instance\\nrecognizer = SpeechRecognizer(engine=engine, vocabulary='english')\\n\\n# Start the recognition process\\nwhile True:\\n    try:\\n        # Get audio input from microphone or file\\n        audio, _ = recognizer.record()\\n        \\n        # Recognize speech using Google Cloud Speech-to-Text API\\n        transcript = recognizer.recognize_speech(audio)\\n        \\n        # Print the recognized text\\n        print(transcript)\\n    except Exception as e:\\n        print('Error:', e)\\n```\\nIn\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Help me write a speech recogniser in python using pyttsx3 and the SpeechRecognition library\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quaternion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
