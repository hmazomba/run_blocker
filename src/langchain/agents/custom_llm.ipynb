{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "huggingfacehub_api_key= os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.schema import AgentAction, AgentFinish, OutputParserException\n",
    "import re\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.tools.file_management.write import WriteFileTool\n",
    "from langchain.tools.file_management.read import ReadFileTool\n",
    "from langchain.utilities import PythonREPL\n",
    "from langchain.tools import DuckDuckGoSearchRun \n",
    "from langchain.tools.file_management import (\n",
    "    ReadFileTool,\n",
    "    CopyFileTool,\n",
    "    DeleteFileTool,\n",
    "    MoveFileTool,\n",
    "    WriteFileTool,\n",
    "    ListDirectoryTool,\n",
    ")\n",
    "from typing import List, Union\n",
    "from langchain.agents.agent_toolkits import FileManagementToolkit\n",
    "from tempfile import TemporaryDirectory\n",
    "from langchain.tools import ShellTool\n",
    "from langchain.agents import load_tools\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make a temporary directory to avoid clutter\n",
    "working_directory = TemporaryDirectory()\n",
    "#Shell Tool\n",
    "shell_tool = ShellTool()\n",
    "requests_tools = load_tools([\"requests_all\"])\n",
    "\n",
    "#DuckDuckGo Class\n",
    "duckduck_search = DuckDuckGoSearchRun()\n",
    "\n",
    "#Python REPL Class\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "shell_tool = Tool(\n",
    "    name=\"shell\",\n",
    "    description=\"This tool gives you access to a system's shell. Can be used to run shell commands\",\n",
    "    func=shell_tool.run\n",
    ")\n",
    "\n",
    "\n",
    "#DuckDuckGo Tool\n",
    "duckduck_tool=Tool(\n",
    "        name = \"DuckDuckGo\",\n",
    "        func=duckduck_search.run,\n",
    "        description=\"useful for when you need to answer questions about current events. You should ask targeted questions\"\n",
    ")\n",
    "\n",
    "#Python REPL Agent Tool Class\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "    func=python_repl.run\n",
    ")\n",
    "toolkit = FileManagementToolkit(root_dir=str(working_directory.name)) # If you don't provide a root_dir, operations will default to the current working directory\n",
    "directory_tools = toolkit.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tools = [\n",
    "    WriteFileTool(),\n",
    "    ReadFileTool(),\n",
    "    repl_tool,\n",
    "    duckduck_tool, \n",
    "    #directory_tools, \n",
    "    shell_tool, \n",
    "    #requests_tool\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(page_content=t.description, metadata={\"index\": i})\n",
    "    for i, t in enumerate(tools)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(docs, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "\n",
    "def get_tools(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return [tools[d.metadata[\"index\"]] for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base template\n",
    "template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Remember to be as creative as possible!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "# Set up a prompt template\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    ############## NEW ######################\n",
    "    # The list of tools available\n",
    "    tools_getter: Callable\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        ############## NEW ######################\n",
    "        tools = self.tools_getter(kwargs[\"input\"])\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join(\n",
    "            [f\"{tool.name}: {tool.description}\" for tool in tools]\n",
    "        )\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in tools])\n",
    "        return self.template.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools_getter=get_tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(\n",
    "            tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ../../models/ggml-gpt4all-j-v1.3-groovy.bin\n",
      "gptj_model_load: loading model from '../../models/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
      "gptj_model_load: n_vocab = 50400\n",
      "gptj_model_load: n_ctx   = 2048\n",
      "gptj_model_load: n_embd  = 4096\n",
      "gptj_model_load: n_head  = 16\n",
      "gptj_model_load: n_layer = 28\n",
      "gptj_model_load: n_rot   = 64\n",
      "gptj_model_load: f16     = 2\n",
      "gptj_model_load: ggml ctx size = 5401.45 MB\n",
      "gptj_model_load: kv self size  =  896.00 MB\n",
      "gptj_model_load: ................................... done\n",
      "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# There are many CallbackHandlers supported, such as\n",
    "# from langchain.callbacks.streamlit import StreamlitCallbackHandler\n",
    "\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "gpt4_llm = GPT4All(model=\"../../../../Models/LLMs/GPT4All/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\", verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain consisting of the LLM and a prompt\n",
    "llm_chain = LLMChain(llm=gpt4_llm, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=tool_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_error(error) -> str:\n",
    "    return str(error)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=_handle_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'langchain' has no attribute 'debug'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent_executor\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mWrite an article which talks about trends in e-learning in Africa. The article should e about 3 pages long and in PDF format. Write it so that it is intriguing, engaging and informational. Try to use Hlumisa Mazomba\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms writing style. You can find him on LinkedIn.Put it in a folder called \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mecommerce_report\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cedric/lib/python3.11/site-packages/langchain/chains/base.py:267\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    270\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/envs/cedric/lib/python3.11/site-packages/langchain/chains/base.py:133\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run the logic of this chain and add to output if desired.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m        to False.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_inputs(inputs)\n\u001b[0;32m--> 133\u001b[0m callback_manager \u001b[39m=\u001b[39m CallbackManager\u001b[39m.\u001b[39;49mconfigure(\n\u001b[1;32m    134\u001b[0m     callbacks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallbacks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, tags, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtags\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    137\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    138\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    139\u001b[0m     inputs,\n\u001b[1;32m    140\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cedric/lib/python3.11/site-packages/langchain/callbacks/manager.py:795\u001b[0m, in \u001b[0;36mCallbackManager.configure\u001b[0;34m(cls, inheritable_callbacks, local_callbacks, verbose, inheritable_tags, local_tags)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    786\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfigure\u001b[39m(\n\u001b[1;32m    787\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    792\u001b[0m     local_tags: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    793\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CallbackManager:\n\u001b[1;32m    794\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Configure the callback manager.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m     \u001b[39mreturn\u001b[39;00m _configure(\n\u001b[1;32m    796\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m    797\u001b[0m         inheritable_callbacks,\n\u001b[1;32m    798\u001b[0m         local_callbacks,\n\u001b[1;32m    799\u001b[0m         verbose,\n\u001b[1;32m    800\u001b[0m         inheritable_tags,\n\u001b[1;32m    801\u001b[0m         local_tags,\n\u001b[1;32m    802\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/cedric/lib/python3.11/site-packages/langchain/callbacks/manager.py:1027\u001b[0m, in \u001b[0;36m_configure\u001b[0;34m(callback_manager_cls, inheritable_callbacks, local_callbacks, verbose, inheritable_tags, local_tags)\u001b[0m\n\u001b[1;32m   1023\u001b[0m tracing_v2_enabled_ \u001b[39m=\u001b[39m (\n\u001b[1;32m   1024\u001b[0m     env_var_is_set(\u001b[39m\"\u001b[39m\u001b[39mLANGCHAIN_TRACING_V2\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m tracer_v2 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m )\n\u001b[1;32m   1026\u001b[0m tracer_session \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mLANGCHAIN_SESSION\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1027\u001b[0m debug \u001b[39m=\u001b[39m _get_debug()\n\u001b[1;32m   1028\u001b[0m \u001b[39mif\u001b[39;00m tracer_session \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1029\u001b[0m     tracer_session \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cedric/lib/python3.11/site-packages/langchain/callbacks/manager.py:72\u001b[0m, in \u001b[0;36m_get_debug\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_debug\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m langchain\u001b[39m.\u001b[39;49mdebug\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'langchain' has no attribute 'debug'"
     ]
    }
   ],
   "source": [
    "agent_executor.run(\"What is the quantum gravity?'\", debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
