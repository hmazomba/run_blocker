{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "huggingfacehub_api_key= os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hmazomba/.conda/envs/cedric/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)e9125/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 710kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 110kB/s]\n",
      "Downloading (…)7e55de9125/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 15.5MB/s]\n",
      "Downloading (…)55de9125/config.json: 100%|██████████| 612/612 [00:00<00:00, 1.38MB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 193kB/s]\n",
      "Downloading (…)125/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 165kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:11<00:00, 8.14MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 78.5kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 116kB/s]\n",
      "Downloading (…)e9125/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 945kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 722kB/s]\n",
      "Downloading (…)9125/train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 10.7MB/s]\n",
      "Downloading (…)7e55de9125/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 19.8MB/s]\n",
      "Downloading (…)5de9125/modules.json: 100%|██████████| 349/349 [00:00<00:00, 254kB/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load PDF file from data path\n",
    "loader = DirectoryLoader('../../../../../Deviare/Documents/',\n",
    "                         glob=\"*.pdf\",\n",
    "                         loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text from PDF into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                               chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Load embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                   model_kwargs={'device': 'cpu'})\n",
    "\n",
    "# Build and persist FAISS vector store\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "vectorstore.save_local('vectorstore/db_faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: prompts.py\n",
    "\n",
    "qa_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers\n",
    "model_path='../../../../../Models/LLMs/GGML/llama-2-7b-chat.ggmlv3.q8_0.bin'\n",
    "# Local CTransformers wrapper for Llama-2-7B-Chat\n",
    "llm = CTransformers(model=model_path, # Location of downloaded GGML model\n",
    "                    model_type='llama', # Model type Llama\n",
    "                    config={'max_new_tokens': 256,\n",
    "                            'temperature': 0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Wrap prompt template in a PromptTemplate object\n",
    "def set_qa_prompt():\n",
    "    prompt = PromptTemplate(template=qa_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Build RetrievalQA object\n",
    "def build_retrieval_qa(llm, prompt, vectordb):\n",
    "    dbqa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=vectordb.as_retriever(search_kwargs={'k':2}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt': prompt})\n",
    "    return dbqa\n",
    "\n",
    "\n",
    "# Instantiate QA object\n",
    "def setup_dbqa():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                       model_kwargs={'device': 'cpu'})\n",
    "    vectordb = FAISS.load_local('vectorstore/db_faiss', embeddings)\n",
    "    qa_prompt = set_qa_prompt()\n",
    "    dbqa = build_retrieval_qa(llm, qa_prompt, vectordb)\n",
    "\n",
    "    return dbqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('input', type=str)\n",
    "args = parser.parse_args()\n",
    "start = timeit.default_timer() # Start timer\n",
    "\n",
    "# Setup QA object\n",
    "dbqa = setup_dbqa()\n",
    "\n",
    "# Parse input from argparse into QA object\n",
    "response = dbqa({'query': \"How much is the minimum guarantee payable by adidas?\"})\n",
    "end = timeit.default_timer() # End timer\n",
    "\n",
    "# Print document QA response\n",
    "print(f'\\nAnswer: {response[\"result\"]}')\n",
    "print('='*50) # Formatting separator\n",
    "\n",
    "# Process source documents for better display\n",
    "source_docs = response['source_documents']\n",
    "for i, doc in enumerate(source_docs):\n",
    "    print(f'\\nSource Document {i+1}\\n')\n",
    "    print(f'Source Text: {doc.page_content}')\n",
    "    print(f'Document Name: {doc.metadata[\"source\"]}')\n",
    "    print(f'Page Number: {doc.metadata[\"page\"]}\\n')\n",
    "    print('='* 50) # Formatting separator\n",
    "    \n",
    "# Display time taken for CPU inference\n",
    "print(f\"Time to retrieve response: {end - start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
