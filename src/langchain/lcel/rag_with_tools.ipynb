{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.chat_models import ChatLiteLLM\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "cohere_api_key= os.getenv('GOOGLE_API_KEY')\n",
    "huggingfacehub_api_key= os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "pdf_loader = DirectoryLoader('../../../../data/tenders', glob=\"**/*.pdf\")\n",
    "txt_loader = DirectoryLoader('../../../../data/tenders', glob=\"**/*.txt\")\n",
    "word_loader = DirectoryLoader('../../../../data/tenders', glob=\"**/*.docx\")\n",
    "csv_loader = DirectoryLoader('../../../../data/tenders', glob=\"**/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain.document_loaders import PlaywrightURLLoader\n",
    "\n",
    "url = \"https://spiderman.fandom.com/wiki\"\n",
    "recursive_url_loader = RecursiveUrlLoader(url=url, max_depth=5, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
    "\n",
    "\n",
    "loaders = [recursive_url_loader]\n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "print(f\"Total number of documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents, HuggingFaceEmbeddings())\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatLiteLLM(model='palm/chat-bison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/litellm/utils.py:3794\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   3788\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3789\u001b[0m         \u001b[39m# if no status code then it is an APIConnectionError: https://github.com/openai/openai-python#handling-errors\u001b[39;00m\n\u001b[1;32m   3790\u001b[0m         \u001b[39mraise\u001b[39;00m APIConnectionError(\n\u001b[1;32m   3791\u001b[0m             __cause__\u001b[39m=\u001b[39moriginal_exception\u001b[39m.\u001b[39m__cause__,\n\u001b[1;32m   3792\u001b[0m             llm_provider\u001b[39m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   3793\u001b[0m             model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m-> 3794\u001b[0m             request\u001b[39m=\u001b[39moriginal_exception\u001b[39m.\u001b[39;49mrequest\n\u001b[1;32m   3795\u001b[0m         )\n\u001b[1;32m   3796\u001b[0m \u001b[39melif\u001b[39;00m custom_llm_provider \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39manthropic\u001b[39m\u001b[39m\"\u001b[39m:  \u001b[39m# one of the anthropics\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OpenAIError' object has no attribute 'request'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/hmazomba/projects/run_blocker/src/langchain/lcel/rag_with_tools.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hmazomba/projects/run_blocker/src/langchain/lcel/rag_with_tools.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m chain\u001b[39m.\u001b[39;49minvoke(\u001b[39m\"\u001b[39;49m\u001b[39mWho is the Green Goblin?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/langchain_core/runnables/base.py:1470\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1469\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1470\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   1471\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1472\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1473\u001b[0m             patch_config(\n\u001b[1;32m   1474\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1475\u001b[0m             ),\n\u001b[1;32m   1476\u001b[0m         )\n\u001b[1;32m   1477\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:160\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    151\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    156\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[1;32m    157\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[1;32m    159\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    161\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[1;32m    162\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    163\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    164\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    165\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    166\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    167\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    168\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[1;32m    169\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:481\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    474\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    475\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    479\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    480\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 481\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:370\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    369\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 370\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    371\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    372\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    373\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    374\u001b[0m ]\n\u001b[1;32m    375\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:360\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    358\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 360\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    361\u001b[0m                 m,\n\u001b[1;32m    362\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    363\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    364\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    365\u001b[0m             )\n\u001b[1;32m    366\u001b[0m         )\n\u001b[1;32m    367\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    368\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:514\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    511\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 514\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    515\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    516\u001b[0m     )\n\u001b[1;32m    517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/langchain/chat_models/litellm.py:307\u001b[0m, in \u001b[0;36mChatLiteLLM._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    306\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m--> 307\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    308\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    309\u001b[0m )\n\u001b[1;32m    310\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/langchain/chat_models/litellm.py:240\u001b[0m, in \u001b[0;36mChatLiteLLM.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    238\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcompletion(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/langchain/chat_models/litellm.py:238\u001b[0m, in \u001b[0;36mChatLiteLLM.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcompletion(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/litellm/utils.py:1364\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1361\u001b[0m             liteDebuggerClient \u001b[39mand\u001b[39;00m liteDebuggerClient\u001b[39m.\u001b[39mdashboard_url \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m         ):  \u001b[39m# make it easy to get to the debugger logs if you've initialized it\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m             e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Check the log in your dashboard - \u001b[39m\u001b[39m{\u001b[39;00mliteDebuggerClient\u001b[39m.\u001b[39mdashboard_url\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1364\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/litellm/utils.py:1293\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1291\u001b[0m                     \u001b[39mreturn\u001b[39;00m cached_result\n\u001b[1;32m   1292\u001b[0m \u001b[39m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1293\u001b[0m result \u001b[39m=\u001b[39m original_function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1294\u001b[0m end_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m   1295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m   1296\u001b[0m     \u001b[39m# TODO: Add to cache for streaming\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/litellm/main.py:1424\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, deployment_id, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1421\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m   1422\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1423\u001b[0m     \u001b[39m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 1424\u001b[0m     \u001b[39mraise\u001b[39;00m exception_type(\n\u001b[1;32m   1425\u001b[0m             model\u001b[39m=\u001b[39;49mmodel, custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider, original_exception\u001b[39m=\u001b[39;49me, completion_kwargs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1426\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/litellm/utils.py:4633\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   4631\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   4632\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4633\u001b[0m     \u001b[39mraise\u001b[39;00m original_exception\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/litellm/main.py:596\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, deployment_id, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    589\u001b[0m     \u001b[39m## LOGGING - log the original exception returned\u001b[39;00m\n\u001b[1;32m    590\u001b[0m     logging\u001b[39m.\u001b[39mpost_call(\n\u001b[1;32m    591\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m    592\u001b[0m         api_key\u001b[39m=\u001b[39mapi_key,\n\u001b[1;32m    593\u001b[0m         original_response\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    594\u001b[0m         additional_args\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m: headers},\n\u001b[1;32m    595\u001b[0m     )\n\u001b[0;32m--> 596\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    598\u001b[0m \u001b[39m## LOGGING\u001b[39;00m\n\u001b[1;32m    599\u001b[0m logging\u001b[39m.\u001b[39mpost_call(\n\u001b[1;32m    600\u001b[0m     \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m    601\u001b[0m     api_key\u001b[39m=\u001b[39mapi_key,\n\u001b[1;32m    602\u001b[0m     original_response\u001b[39m=\u001b[39mresponse,\n\u001b[1;32m    603\u001b[0m     additional_args\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m: headers},\n\u001b[1;32m    604\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/litellm/main.py:572\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, deployment_id, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[39m## COMPLETION CALL\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 572\u001b[0m     response \u001b[39m=\u001b[39m openai_chat_completions\u001b[39m.\u001b[39;49mcompletion(\n\u001b[1;32m    573\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    574\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m    575\u001b[0m         model_response\u001b[39m=\u001b[39;49mmodel_response,\n\u001b[1;32m    576\u001b[0m         print_verbose\u001b[39m=\u001b[39;49mprint_verbose,\n\u001b[1;32m    577\u001b[0m         api_key\u001b[39m=\u001b[39;49mapi_key,\n\u001b[1;32m    578\u001b[0m         api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[1;32m    579\u001b[0m         acompletion\u001b[39m=\u001b[39;49macompletion,\n\u001b[1;32m    580\u001b[0m         logging_obj\u001b[39m=\u001b[39;49mlogging,\n\u001b[1;32m    581\u001b[0m         optional_params\u001b[39m=\u001b[39;49moptional_params,\n\u001b[1;32m    582\u001b[0m         litellm_params\u001b[39m=\u001b[39;49mlitellm_params,\n\u001b[1;32m    583\u001b[0m         logger_fn\u001b[39m=\u001b[39;49mlogger_fn,\n\u001b[1;32m    584\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    585\u001b[0m         custom_prompt_dict\u001b[39m=\u001b[39;49mcustom_prompt_dict,\n\u001b[1;32m    586\u001b[0m         client\u001b[39m=\u001b[39;49mclient \u001b[39m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[1;32m    587\u001b[0m     )\n\u001b[1;32m    588\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    589\u001b[0m     \u001b[39m## LOGGING - log the original exception returned\u001b[39;00m\n\u001b[1;32m    590\u001b[0m     logging\u001b[39m.\u001b[39mpost_call(\n\u001b[1;32m    591\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m    592\u001b[0m         api_key\u001b[39m=\u001b[39mapi_key,\n\u001b[1;32m    593\u001b[0m         original_response\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    594\u001b[0m         additional_args\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m: headers},\n\u001b[1;32m    595\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/litellm/llms/openai.py:252\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, model, messages, print_verbose, api_key, api_base, acompletion, logging_obj, optional_params, litellm_params, logger_fn, headers, custom_prompt_dict, client)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    251\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e: \n\u001b[0;32m--> 252\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/litellm/llms/openai.py:247\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, model, messages, print_verbose, api_key, api_base, acompletion, logging_obj, optional_params, litellm_params, logger_fn, headers, custom_prompt_dict, client)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 messages \u001b[39m=\u001b[39m new_messages\n\u001b[1;32m    246\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m                 \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    248\u001b[0m \u001b[39mexcept\u001b[39;00m OpenAIError \u001b[39mas\u001b[39;00m e: \n\u001b[1;32m    249\u001b[0m     exception_mapping_worked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/litellm/llms/openai.py:218\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, model, messages, print_verbose, api_key, api_base, acompletion, logging_obj, optional_params, litellm_params, logger_fn, headers, custom_prompt_dict, client)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[39mraise\u001b[39;00m OpenAIError(status_code\u001b[39m=\u001b[39m\u001b[39m422\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax retries must be an int\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m client \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     openai_client \u001b[39m=\u001b[39m OpenAI(api_key\u001b[39m=\u001b[39;49mapi_key, base_url\u001b[39m=\u001b[39;49mapi_base, http_client\u001b[39m=\u001b[39;49mlitellm\u001b[39m.\u001b[39;49mclient_session, timeout\u001b[39m=\u001b[39;49mtimeout, max_retries\u001b[39m=\u001b[39;49mmax_retries)\n\u001b[1;32m    219\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     openai_client \u001b[39m=\u001b[39m client\n",
      "File \u001b[0;32m~/anaconda3/envs/cedric/lib/python3.10/site-packages/openai/_client.py:93\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     91\u001b[0m     api_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m api_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m api_key\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m organization \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "chain.invoke(\"Who is the Green Goblin?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
